{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZF Full Text Search\n",
    "\n",
    "*Eddie Chapman*\n",
    "\n",
    "Tokenizes text from multiple text files and stores the results in a CSV file.\n",
    "\n",
    "Part of The Zuckerberg Files project. \n",
    "\n",
    "https://alir3z4.github.io/stop-words/\n",
    "\n",
    "### Input: text files\n",
    "- Multiple, located in the same folder\n",
    "- Filenames: the file's corresponding Zuck Files record ID\n",
    "- Contain metadata at the top of the file and transcript contents below\n",
    "- Metadata fields, main content, and individual speakers are designated with double hashtags (`##title##`)\n",
    "- We want to skip the metadata and tokenize everything after the `##content##` tag, except for speaker names\n",
    "\n",
    "### Output: CSV\n",
    "- Each row corresponds to a single tokenized text file\n",
    "- `record_id` field lists the ID which is also the .txt filename\n",
    "- `full_text` field contains the text file's contents, tokenized\n",
    "- tokenized means:\n",
    "    + Starting *below* the `##contents##` field in the text file\n",
    "    + Skipping all info in hashtags, such as speaker names\n",
    "    + Skipping all bracketted text\n",
    "    + Removing stop words\n",
    "    + Removing punctuation (except for symbols located within words such as apostrophes)\n",
    "    + All remaining words joined into a string, seperated by spaces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are your text files?\n",
    "\n",
    "Modify the address below. Remember, Windows uses single slashes to seperate the directory. Python needs you to double them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\chapman4\\\\Downloads\\\\zuck-text-ready-test\\\\text\\\\text-new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a list of all of the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_filenames():\n",
    "    filenames = [name.split(\".\")[0] for name in os.listdir(\".\") if name.endswith(\".txt\")]\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the output CSV going to be called?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILE_FULL_TEXT = 'zuck_full_text.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's a list of stop words to ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_stopwords():\n",
    "    with open('stopwords.txt', 'r') as infile:\n",
    "        stopwords = [line.strip() for line in infile]\n",
    "        return stopwords\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are those text files encoded?\n",
    "\n",
    "This tries to acknowledge the multiple encodings of the text files. I'm not sure if it really helps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Detector object for encoding detection\n",
    "def detect_encoding(filename):\n",
    "    detector = UniversalDetector()\n",
    "    with open(filename + '.txt', 'rb') as infile:\n",
    "        for line in infile:\n",
    "            detector.feed(line)\n",
    "            if detector.done:   # detection process ends automatically when confidence is high enough\n",
    "                break\n",
    "        detector.close()\n",
    "        return detector.result['encoding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grab_text(filenames, stopwords): \n",
    "    rows = []\n",
    "    for filename in filenames:\n",
    "        encoding = detect_encoding(filename)\n",
    "        with codecs.open(filename + '.txt', 'r', encoding) as infile: \n",
    "            row = {}\n",
    "            text = infile.read()\n",
    "            tokens_nonnormal = re.findall(\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\", text)\n",
    "            tokens_normal = [t.lower() for t in tokens_nonnormal]\n",
    "            tokens_minus_stop = [t for t in tokens_normal if t not in stopwords]\n",
    "            tokens = ' '.join(tokens_minus_stop)\n",
    "            row['record_id'] = filename\n",
    "            row['full_text'] = tokens\n",
    "            rows.append(row)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write to csv file\n",
    "def write_csv(filename, rows):\n",
    "    with open(filename, 'w', encoding='utf-8') as csv_file:\n",
    "        col_names = rows[0].keys()\n",
    "        writer = csv.DictWriter(csv_file, col_names)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    filenames = list_filenames()\n",
    "    stopwords = list_stopwords()\n",
    "    rows = grab_text(filenames, stopwords)\n",
    "    write_csv(FILE_FULL_TEXT, rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
